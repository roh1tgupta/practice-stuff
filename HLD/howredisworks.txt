Cloud-Hosted Redis Services (e.g., AWS ElastiCache, Redis Enterprise Cloud, Google Cloud Memorystore) use Redis Cluster mode for multi-node scalability, high availability, and fault tolerance. Data is sharded across nodes, with replication for redundancy, and client-side routing ensures efficient request handling without central tracking.
1. Overview of Multi-Node Redis

Clustering Basics: Data distributed (sharded) across nodes; cluster appears unified to apps.
Key Components:

Sharding: Partitions data horizontally.
Replication: Copies data to secondary nodes.
Client-Side Routing: App clients (e.g., Node.js redis/ioredis) route to correct nodes.


Cloud providers manage setup, monitoring, failover, scaling (e.g., AWS ElastiCache spreads key space across shards for even distribution/low latency).
Requests use deterministic key-based routing, not central tracking.

2. Sharding (Data Distribution)

Key space divided into 16,384 fixed hash slots.
Slot calculation: CRC16(key) % 16384 determines slot.
Slots assigned to nodes; cloud services auto-balance/reassign during scaling/failures.
Data stored on owning node (and replicas); not duplicated across all nodes to save memory.
If request hits wrong node, it responds with MOVED redirection; smart clients cache/update slot-to-node map for direct future routing.
Example: First request (cache miss) hashes to Slot X on Server 1 → Fetch from MongoDB → Cache on Server 1. Next: Client routes to Server 1 for hit. Wrong initial route (e.g., outdated map): Server 2 redirects to Server 1, client updates.
Ensures consistency: Same key always to same node (unless resharded, client adapts).

3. Replication (State Maintenance/Availability)

Asynchronous: Primary node writes, copies to 1+ replicas (slaves).
High availability: Replica promoted on primary failure (cloud-handled, seconds-minutes).
Replicas offload reads.
Consistency: Eventual by default (ms lag); use WAIT for stronger (trades speed).
Cloud specifics:

AWS ElastiCache: Multi-AZ replication for durability; DNS updates on failover.
Redis Enterprise Cloud: Shared-nothing architecture; active-active for global.


No full duplication across nodes—only within shard's replicas.
Caching example: Data on Server 1 primary replicated to replicas; failure → Replica promotes, app accesses seamlessly (redirections/DNS).
Reads configurable from replicas; writes to primary.
Low latency (~sub-ms hits); state preserved.

4. Handling Sessions

Session keys (e.g., session:abc123) hashed to slot/node like any key.
App (e.g., Express.js with connect-redis) uses cluster-aware client for routing.
Operations for a session ID stay on same node; no cross-node comms.
Failure: Replication ensures data on promoted replica.
Multi-node apps: All web servers share cluster endpoint; client handles routing.
HA: Enable 1-5 replicas per primary; sessions persist on failovers.
Security/Isolation: Use separate DBs (Redis supports 16) or prefixes; prefer dedicated clusters.
Cloud features:

AWS ElastiCache: Auto-failover, multi-AZ for e-commerce sessions (e.g., carts).
Redis Enterprise: Active-active geo-replication, zero-downtime scaling.


Latency: ~1ms or less; use TTL for expiration.
Node.js example:
javascriptconst Redis = require('ioredis');
const client = new Redis.Cluster([{ host: 'your-elasticache-endpoint', port: 6379 }]);
await client.set('session:abc123', JSON.stringify({ user: 'data' })); // Hashes to node
const session = await client.get('session:abc123'); // Routes to same


5. Challenges & Best Practices

Cache Misses/Fetches: App fetches from MongoDB on miss; deterministic sharding minimizes repeats unless evicted (use LRU/LFU).
Resharding: Slots reassigned on scaling; clients handle via MOVED, minimal disruption.
Consistency Trade-offs: Sync modes for strong needs (higher latency).
Monitoring: Cloud metrics (hits/misses, lag) via CloudWatch/Redis dashboards.
Non-Seamless Cases: Use cluster-aware clients (e.g., ioredis over basic redis).
Alternatives: Redis with CRDTs for large-scale/geo sessions.

Summary: Redis maintains state via sharding (distribution) and replication (redundancy); clients route smartly to avoid hops. For URL shortener, use cluster-aware Node.js client (ioredis for ElastiCache). Dive deeper on providers if needed.
Additional Concerns Addressed

Concern 1 (Cache Miss & Redirection): Nodes aware via gossip protocol (periodic metadata exchange); all know full slot-to-node map. Wrong node computes slot, checks state, sends MOVED with correct address. Client updates/retries. Miss/fetch/cache on correct node. Efficient: Decentralized, rare redirections post-warmup. Cloud handles sync/monitoring.
Concern 2 (Node.js Client Limits): Client maintains slot-to-node map (not per-key), fixed ~100-200 KB for 16,384 slots—scales regardless of millions/billions keys. Builds on connect (queries node for state); updates incrementally via MOVED/ASK. ioredis: Auto-discovers, pools connections, retries. Global apps: Per-instance maps, no bottleneck; cloud scales cluster.
Hash vs. Exact Key: Hash (CRC16) for slot/route only; exact key for node lookup (O(1) hash table). Fixed slots balance efficiency/granularity. Production: ioredis with auth/retries; cache miss example code provided.